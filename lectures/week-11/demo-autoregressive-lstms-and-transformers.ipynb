{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aa3f80",
   "metadata": {
    "id": "eWeG0UmMs_OO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801e0e71",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "source": [
    "\n",
    "- This notebook uses a project Guttenber text \n",
    "- Try replacing that with the Epicurious Recipes Dataset\n",
    "```bash\n",
    "bash scripts/download_kaggle_data.sh hugodarwood epirecipes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5b51dc",
   "metadata": {
    "id": "1lZ5LCr5tX-_",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "def download_corpus(url, filename):\n",
    "    if not os.path.exists(filename):\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "corpus_url = \"https://gutenberg.org/ebooks/11.txt.utf-8\"\n",
    "corpus_filename = \"alice_in_wonderland.txt\"\n",
    "download_corpus(corpus_url, corpus_filename)\n",
    "\n",
    "with open(corpus_filename, 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a060ec5",
   "metadata": {
    "id": "482n9YUktd6C",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Strip out non-ASCII characters\n",
    "text = text.encode('ascii', 'ignore').decode('utf-8')\n",
    "\n",
    "def split(text):\n",
    "  text = text.lower()\n",
    "  for punct in ',.!?\\'\"-;:':\n",
    "    text = text.replace(punct, ' '+punct+' ')\n",
    "  # Should be smarter but I am impatient for this demo\n",
    "  return text\n",
    "\n",
    "text = split(text)\n",
    "\n",
    "# Tokenize the text (whitespace boundaries, not BPE)\n",
    "tokens = text.split()\n",
    "vocab = ['<BOS>', '<UNK>', '<PAD>', '<EOS>'] + list(set(tokens) )\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Map tokens to indices\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {i: word for word, i in word_to_ix.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af361c6d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9U6VfkEMt1xn",
    "outputId": "a3753325-c532-411d-af83-4a53e4873c1d"
   },
   "outputs": [],
   "source": [
    "word_to_ix['alice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02090050",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "NHvjb2rXt5LT",
    "outputId": "25afeea0-bdcc-4a46-9183-fe089929d056"
   },
   "outputs": [],
   "source": [
    "ix_to_word[_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7def276c",
   "metadata": {
    "id": "4qwwlXiYuzdD"
   },
   "outputs": [],
   "source": [
    "BOS = word_to_ix['<BOS>']\n",
    "PAD = word_to_ix['<PAD>']\n",
    "UNK = word_to_ix['<UNK>']\n",
    "EOS = word_to_ix['<EOS>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d81d18",
   "metadata": {
    "id": "JNzq2TWItmzS",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Define sequence length\n",
    "seq_length = 50\n",
    "sequences = []\n",
    "targets = []\n",
    "\n",
    "# Essentially the ngrams, but with a much larger context\n",
    "for i in range(len(tokens) - seq_length-1):\n",
    "    seq = ['<BOS>'] + tokens[i:i + seq_length-1]\n",
    "    target = tokens[i:i + seq_length] # Input shifted left\n",
    "    sequences.append([word_to_ix[word] for word in seq])\n",
    "    targets.append([word_to_ix[word] for word in target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68cf85b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uaruADPvtzlg",
    "outputId": "8e0065e7-cd91-4dda-fc56-4ade34be4e74"
   },
   "outputs": [],
   "source": [
    "print('Context: ', sequences[4000])\n",
    "print('Target: ', targets[4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68a0c7f",
   "metadata": {
    "id": "Jz-QLMy8uIcx"
   },
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "  text = split(text)\n",
    "  return [word_to_ix.get(wd, 0) for wd in text.split()]\n",
    "\n",
    "def decode(sequence):\n",
    "  return ' '.join([ix_to_word[ix] for ix in sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a38e950",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0vGMRrJnwI91",
    "outputId": "49eea4d1-9a98-4fe0-cac1-433019275aa6"
   },
   "outputs": [],
   "source": [
    "encode(\"well, children, what's up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a460a3e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "oJbuycrGwPJs",
    "outputId": "6647aeb4-cb99-4d71-adc0-fb1528ab9fbe"
   },
   "outputs": [],
   "source": [
    "decode(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcd8304",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "po2JnMnb0a7L",
    "outputId": "76e31af0-1b5c-497d-a3ae-3a6813ceb607"
   },
   "outputs": [],
   "source": [
    "decode(sequences[4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc0609b",
   "metadata": {
    "id": "XJ1iQWd80kx0"
   },
   "outputs": [],
   "source": [
    "sequences = torch.tensor(sequences)\n",
    "targets = torch.tensor(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bd5e98",
   "metadata": {
    "id": "409GOLmBy0wp"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80901dc6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XGr40vE6zMV0",
    "outputId": "4e0aca19-33cf-45f5-8815-db57108f4713"
   },
   "outputs": [],
   "source": [
    "print(embedding.weight.shape)\n",
    "print(\"Words:\", len(vocab))\n",
    "print(\"Embedding of 'alice':\")\n",
    "print(\"   Index: \", word_to_ix['alice'])\n",
    "print(\"   Vector:\", embedding(torch.tensor([[word_to_ix['alice']]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995368c5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "id": "ZgE0TZ1f0Vtz",
    "outputId": "233c646e-95db-42f4-e3c5-c8213c318c20"
   },
   "outputs": [],
   "source": [
    "print('Embedding shape: ', embedding(sequences[4000]).shape)\n",
    "\n",
    "from matplotlib.pylab import plt\n",
    "\n",
    "plt.imshow(embedding(sequences[4000]).detach(), cmap='gray')\n",
    "plt.ylabel('token index')\n",
    "plt.xlabel('embedding')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf5e30a",
   "metadata": {
    "id": "RrPORHRczPXT"
   },
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f63aa9",
   "metadata": {
    "id": "n7ECO-341N81"
   },
   "outputs": [],
   "source": [
    "outputs, (h, c) = lstm(embedding(sequences[4000].unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7bb2eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uRc01hFR1WTr",
    "outputId": "a514f81f-f629-4003-ded3-d61c1bdd2d6b"
   },
   "outputs": [],
   "source": [
    "print(outputs.shape)  # N, L, hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21f7b83",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eFsmJBUR16Zs",
    "outputId": "0b23ff42-bfc6-42d5-9025-470ce13c64aa"
   },
   "outputs": [],
   "source": [
    "print(h.shape) # num_layers, N, hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17555784",
   "metadata": {
    "id": "6-w13i1A2LjR"
   },
   "outputs": [],
   "source": [
    "fc = nn.Linear(hidden_dim, vocab_size)  # hidden-> word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f34544",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7diCo64u2ZfA",
    "outputId": "37f68731-6ac5-4110-abd7-fa754640ee4a"
   },
   "outputs": [],
   "source": [
    "lg_probs = fc(outputs)\n",
    "wds = lg_probs.argmax(1)\n",
    "wds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fcfba0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "yAdv1gjj3NNz",
    "outputId": "ccf1724c-85d9-4278-9924-9149bb7c20eb"
   },
   "outputs": [],
   "source": [
    "decode(wds.squeeze(0).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc27de64",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zV3nh0xi5i2e",
    "outputId": "e701b07b-8c69-4da2-b032-010ce3edfd93"
   },
   "outputs": [],
   "source": [
    "# lg_probs.shape - N, L, C\n",
    "# targets.shape - N, L\n",
    "# cross_entropy expects:   N, C, L  for logits.\n",
    "\n",
    "loss = torch.nn.functional.cross_entropy(lg_probs.permute(0, 2, 1), targets[4000].unsqueeze(0))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57537821",
   "metadata": {
    "id": "Zkr2DtFs3VuJ",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Putting it together\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "lstm_model = LSTMModel(vocab_size, embedding_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f51ac8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sgqk11HD4Chf",
    "outputId": "e7277fea-37e3-4939-c8bc-f3664295a53d"
   },
   "outputs": [],
   "source": [
    "lstm_model(sequences[4000].unsqueeze(0)).argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1be1c3",
   "metadata": {
    "id": "AhCvxdaS3vwL"
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0223608e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2e9fMrIH4fMX",
    "outputId": "347f887b-e1e6-4628-85aa-5338021c5ddc"
   },
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(sequences), 32):  # Batch size of 32\n",
    "        inputs = sequences[i:i+32]\n",
    "        labels = targets[i:i+32]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = lstm_model(inputs)\n",
    "        loss = loss_fn(outputs.permute(0,2,1), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f9cdae",
   "metadata": {
    "id": "HSmOfgbo67OB"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_text='', length=100, temperature=1.0):\n",
    "    model.eval()\n",
    "    input_seq = [BOS]+encode(start_text)\n",
    "    input_seq = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    for _ in range(length):\n",
    "        with torch.no_grad():\n",
    "            output = model(input_seq)\n",
    "            output = output[:, -1, :]  # Take last output of the sequence\n",
    "            output = output / temperature  # Apply temperature\n",
    "            probabilities = torch.nn.functional.softmax(output, dim=-1).squeeze()\n",
    "\n",
    "            next_word_id = torch.multinomial(probabilities, 1).item()\n",
    "            next_word = ix_to_word[next_word_id]\n",
    "\n",
    "            yield next_word, probabilities\n",
    "\n",
    "            input_seq = torch.cat((input_seq, torch.tensor([[next_word_id]])), dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a8f034",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W-pidjInLiHk",
    "outputId": "4a13387b-caca-4af3-9ab2-23d113ab8457"
   },
   "outputs": [],
   "source": [
    "line = []\n",
    "for w, p in generate_text(lstm_model):\n",
    "  line += ' '\n",
    "  line += w\n",
    "  if len(line) > 60:\n",
    "    print(''.join(line))\n",
    "    line = []\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2afb9f",
   "metadata": {
    "id": "-yRbKwifMs2I"
   },
   "source": [
    "(It's okay that we still get nonesense, it _is_ alice in wonderland. More importantly, it takes a LOT more training to get good results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef87bc6d",
   "metadata": {
    "id": "-GkFqEq5NWwg"
   },
   "source": [
    "# Substituting a transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef6daf7",
   "metadata": {
    "id": "ZHEaDWyBNdrm"
   },
   "outputs": [],
   "source": [
    "# Repeating the embedding snipped above -- to remind ourselves\n",
    "embedding_dim = 128\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b16cba4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "33--fmCcl_ak",
    "outputId": "4049f9e7-ad07-4430-9f4b-2c7be0fb9f02"
   },
   "outputs": [],
   "source": [
    "# We will be doing a GPT style decoder only, however we will be using the pytorch _encoder_ class\n",
    "# This is because the pytorch decoder includes cross-attention, which GPT style decoders dont use\n",
    "# Also,  a decoder is essentially an encoder with masked attention\n",
    "\n",
    "def generate_causal_mask(size):\n",
    "    # Creates a triangular mask that blocks future tokens\n",
    "    mask = torch.triu(torch.ones(size, size) * float('-inf'), diagonal=1)\n",
    "    return mask\n",
    "\n",
    "plt.figure(facecolor='w')\n",
    "plt.imshow(torch.exp(generate_causal_mask(10)), cmap='gray', vmin=0, vmax=1)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title('Causal Mask (w=1, k=0)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4700735",
   "metadata": {
    "id": "lM6L8eExoyDE"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_len=200):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, embedding_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-np.log(10000.0) / embedding_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(1))  # Shape (max_len, 1, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure `pe` matches the input length in sequence dimension\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "pos_encoder = PositionalEncoding(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49b5b7d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "id": "aEAflnBuOwBa",
    "outputId": "dba0e378-af59-485b-de15-e08ab58b35d7"
   },
   "outputs": [],
   "source": [
    "plt.imshow(pos_encoder.pe.detach().squeeze())  # This is what is added to each embedding\n",
    "plt.yticks([0,200])\n",
    "plt.xticks([0,embedding_dim])\n",
    "plt.ylabel('Position (index) in input')\n",
    "plt.xlabel('Embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc74e18",
   "metadata": {
    "id": "4qQFGewJo90O"
   },
   "outputs": [],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=8)\n",
    "encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394afabe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pcAh2zo5pKdH",
    "outputId": "2ba3288c-507e-44fd-a0e2-4865a7807245"
   },
   "outputs": [],
   "source": [
    "encoder(embedding(sequences[4000].unsqueeze(0))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb269b2",
   "metadata": {
    "id": "Qttx12yxNnXs"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_layers=2, nhead=8, max_len=200):\n",
    "        super(TransformerEncoderModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_encoder = PositionalEncoding(embedding_dim, max_len)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embedding_dim, vocab_size)  # Output layer to predict next token\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x) * np.sqrt(x.size(-1))\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x, mask=mask)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "transformer_model = TransformerEncoderModel(vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfad818c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HKt3EYzSSZtC",
    "outputId": "c3c200ba-966e-44de-81f7-e75375864c32"
   },
   "outputs": [],
   "source": [
    "lg_probs = transformer_model(sequences[4000].unsqueeze(0))\n",
    "\n",
    "lg_probs.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b207c8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "OFcecqrrOn5v",
    "outputId": "ba86a987-8229-44d2-92a4-22e0069977b5"
   },
   "outputs": [],
   "source": [
    "decode(lg_probs.squeeze(0).argmax(1).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd2ccb0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bp5cV7PYQRZg",
    "lines_to_next_cell": 2,
    "outputId": "c53db4ee-7150-4243-9b92-8ad81ee49d13"
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "optimizer = optim.Adam(transformer_model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(num_epochs):  # Number of epochs\n",
    "    for i in range(0, len(sequences), 32):  # Batch size of 32\n",
    "        inputs = sequences[i:i+32]\n",
    "        labels = targets[i:i+32]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        causal_mask = generate_causal_mask(inputs.size(0)).to(inputs.device)\n",
    "        outputs = transformer_model(inputs, mask=causal_mask)\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, vocab_size), labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763fdb36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y5mpcGLHN-lX",
    "outputId": "056625f1-8b35-4d16-ab7d-0c14dce24eb1"
   },
   "outputs": [],
   "source": [
    "line = []\n",
    "for w, p in generate_text(transformer_model):\n",
    "  line += ' '\n",
    "  line += w\n",
    "  if len(line) > 60:\n",
    "    print(''.join(line))\n",
    "    line = []\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29baa8a7",
   "metadata": {
    "id": "R5uvctfRAHws"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "py:percent,ipynb",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
