{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2cd5882",
   "metadata": {},
   "source": [
    "# Advanced Techniques for Text Generation with LangChain\n",
    "Demonstrating the use of LangChain for complex text generation tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd52f47",
   "metadata": {},
   "source": [
    "## Why LangChain?\n",
    "- Abstraction over multiple LLM APIs (GPT, LLaMA, Claude, etc.)\n",
    "- Managing complex multi-step workflows with agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865d8557",
   "metadata": {},
   "source": [
    "## Installing LangChain and OpenAI\n",
    "Make sure to install necessary dependencies.\n",
    "```\n",
    "pip install langchain openai\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb05422",
   "metadata": {},
   "source": [
    "## Setting Up API Keys\n",
    "Store API keys securely using environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cdf5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f91ac91",
   "metadata": {},
   "source": [
    "## Loading a Chat Model\n",
    "Example: Loading an OpenAI chat model with LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd228ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "chat = ChatOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558d8824",
   "metadata": {},
   "source": [
    "## Creating an Agent\n",
    "Define an agent with specific tools and a task-focused context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d184594",
   "metadata": {},
   "source": [
    "## Loading a Model Provider\n",
    "In this example, we'll use OpenAI's GPT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82e2e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d607157b",
   "metadata": {},
   "source": [
    "## Managing API Keys (Outside Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c05268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f5bb5b",
   "metadata": {},
   "source": [
    "## Example: Loading OpenAI GPT model\n",
    "\n",
    "```python\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "chat = ChatOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d22f3e",
   "metadata": {},
   "source": [
    "## Creating a Joke Generator\n",
    "System message and human prompt for joke generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1b31ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that tells jokes.\"),\n",
    "    HumanMessage(content=\"Tell me a joke about software engineers.\")\n",
    "]\n",
    "response = chat.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e59bb53",
   "metadata": {},
   "source": [
    "## Streaming Chat Models\n",
    "Demonstrating how to stream responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1b9a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chat.stream(messages):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9729d7e6",
   "metadata": {},
   "source": [
    "## Using Prompt Templates for Dynamic Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa99e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are a creative consultant brainstorming names for businesses.\n",
    "You must follow the following principles:\n",
    "{principles}\n",
    "Please generate a numerical list of five catchy names for a start-up in the {industry} industry that deals with {context}.\n",
    "\"\"\"\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f35078",
   "metadata": {},
   "source": [
    "## Batch API Requests with `.batch()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0950adff",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat.batch(messages=[messages, messages])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614ed6f8",
   "metadata": {},
   "source": [
    "## Asynchronous API Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31784c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def async_invoke():\n",
    "    result = await chat.ainvoke(messages)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce41f861",
   "metadata": {},
   "source": [
    "## Pydantic Model for Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99e0088",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class BusinessName(BaseModel):\n",
    "    name: str\n",
    "    rating_score: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66731a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PydanticOutputParser(pydantic_model=BusinessName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9627f4",
   "metadata": {},
   "source": [
    "## Chaining with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d012ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import SystemMessagePromptTemplate\n",
    "from langchain.chains import SimpleChain\n",
    "\n",
    "template = \"\"\"\n",
    "You are an expert financial assistant. Categorize the following transaction description.\n",
    "\n",
    "Transaction: {transaction_description}\n",
    "\n",
    "Provide the transaction_type and transaction_category.\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327142ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"transaction_description\"],\n",
    "    template=template,\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d0fab8",
   "metadata": {},
   "source": [
    "## Handling Large Text Chunks with Text Splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e799ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "docs = text_splitter.create_documents([\"Sample text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266a8226",
   "metadata": {},
   "source": [
    "## Summarization Chain (Map-Reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5918c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "chain = load_summarize_chain(llm=chat, chain_type=\"map_reduce\")\n",
    "summary = chain.invoke(docs)\n",
    "print(summary['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88425089",
   "metadata": {},
   "source": [
    "## Function Calling with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7294ea49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def schedule_meeting(date, time, attendees):\n",
    "    return {\n",
    "        \"event_id\": \"1234\",\n",
    "        \"status\": \"Meeting scheduled successfully!\",\n",
    "        \"date\": date,\n",
    "        \"time\": time,\n",
    "        \"attendees\": attendees\n",
    "    }\n",
    "\n",
    "functions = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"type\": \"object\",\n",
    "            \"name\": \"schedule_meeting\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"date\": {\"type\": \"string\", \"format\": \"date\"},\n",
    "                    \"time\": {\"type\": \"string\", \"format\": \"time\"},\n",
    "                    \"attendees\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                },\n",
    "                \"required\": [\"date\", \"time\", \"attendees\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d597fa4c",
   "metadata": {},
   "source": [
    "## Example: Scheduling a Meeting with OpenAI and LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4f5f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Schedule a meeting on 2023-11-01 at 14:00 with Alice and Bob.\"}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(model=\"gpt-3.5-turbo-1106\", messages=messages, tools=functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff26305",
   "metadata": {},
   "source": [
    "## Handling Function Calls in Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db219b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if response.choices[0].message.tool_calls:\n",
    "    first_tool_call = response.tool_calls[0]\n",
    "    function_name = first_tool_call.function.name\n",
    "    function_args = json.loads(first_tool_call.function.arguments)\n",
    "    function_response = schedule_meeting(**function_args)\n",
    "\n",
    "    messages.append(\n",
    "        {\"role\": \"function\", \"name\": \"schedule_meeting\", \"content\": json.dumps(function_response)}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec29914",
   "metadata": {},
   "source": [
    "## Text Summarization Using Map-Reduce Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04201b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\"scenes\": [\"Scene 1 text\", \"Scene 2 text\"]})\n",
    "all_text = \"\\n\".join(df.scenes.tolist())\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=1500, chunk_overlap=200)\n",
    "docs = text_splitter.create_documents([all_text])\n",
    "\n",
    "chain = load_summarize_chain(llm=chat, chain_type=\"map_reduce\")\n",
    "summary = chain.invoke(docs)\n",
    "print(summary['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dff3b0",
   "metadata": {},
   "source": [
    "## Final Example: Query Planning with LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ded9d95",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Query(BaseModel):\n",
    "    id: int\n",
    "    question: str\n",
    "    dependencies: list = Field(default_factory=list)\n",
    "\n",
    "class QueryPlan(BaseModel):\n",
    "    query_graph: list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c210888",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PydanticOutputParser(pydantic_object=QueryPlan)\n",
    "template = \"\"\"Generate a query plan for the following task: {query}\n",
    "Return the graph in the following format: {format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "chat_prompt = PromptTemplate.from_messages([system_message_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee19782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chat_prompt | chat | parser\n",
    "\n",
    "result = chain.invoke({\n",
    "    \"query\": \"I want to get results from my database, find the average age of top 10 customers, and send an email to John.\",\n",
    "    \"format_instructions\": parser.get_format_instructions()\n",
    "})\n",
    "\n",
    "print(result.query_graph)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
