{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55243bc1",
   "metadata": {
    "executionInfo": {
     "elapsed": 7982,
     "status": "ok",
     "timestamp": 1726497959919,
     "user": {
      "displayName": "John Femiani",
      "userId": "01158245387658263084"
     },
     "user_tz": 240
    },
    "id": "f9b6d65c",
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "%pip install langchain langchain_openai pandas tqdm --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2abe5d2",
   "metadata": {
    "id": "02ace2b8",
    "lines_to_next_cell": 0
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa13f0d7",
   "metadata": {
    "id": "02ace2b8"
   },
   "source": [
    "## Generate Ground Truth Data with GPT-4 API\n",
    "\n",
    "Creating the known labels or ground truth data can be time consuming and expensive. You can use GPT-4 to _generate the ground truth data_ for you. This is useful for training your own models, and for evaluating the performance of other models. Then you can use these evals to test whether the open source or smaller / faster / cheaper models are performing as well as the larger / slower / more expensive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a69e556",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 606,
     "status": "ok",
     "timestamp": 1726497960519,
     "user": {
      "displayName": "John Femiani",
      "userId": "01158245387658263084"
     },
     "user_tz": 240
    },
    "id": "22708c16",
    "outputId": "39b7099d-0a21-4e4d-b093-35cecb043860"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import io\n",
    "\n",
    "# Dataset URL:\n",
    "url = \"https://storage.googleapis.com/oreilly-content/transaction_data_with_expanded_descriptions.csv\"\n",
    "\n",
    "# Download the file from the URL:\n",
    "downloaded_file = requests.get(url)\n",
    "\n",
    "# Load the transactions dataset and only look at 20 transactions:\n",
    "df = pd.read_csv(io.StringIO(downloaded_file.text))[:20]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516c8683",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13866f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096883bc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0da3464e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c85f633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Define GPT-4 Model for ground truth generation\n",
    "gpt4_model = ChatOpenAI(model=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f37be1a",
   "metadata": {},
   "source": [
    "## Part 1: Setup the OpenAI GPT-4 Model for Ground Truth Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0672d51",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "#### Defining a Structured Output Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067c2b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Union, Literal\n",
    "\n",
    "# Define Pydantic model for transaction classification\n",
    "class EnrichedTransactionInformation(BaseModel):\n",
    "    transaction_type: Union[\n",
    "        Literal[\"Purchase\", \"Withdrawal\", \"Deposit\", \"Bill Payment\", \"Refund\"], None\n",
    "    ]\n",
    "    transaction_category: Union[\n",
    "        Literal[\"Food\", \"Entertainment\", \"Transport\", \"Utilities\", \"Rent\", \"Other\"],\n",
    "        None,\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e03cf2",
   "metadata": {},
   "source": [
    "\n",
    "To improve our model's accuracy, we'll define a more structured output model using Pydantic. This model will specify the exact values that our transaction types and categories can take, helping to constrain the model's outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45864704",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "In this code block, we've defined a Pydantic model `EnrichedTransactionInformation` that specifies the exact values our `transaction_type` and `transaction_category` fields can take.\n",
    "\n",
    "The use of `Union` and `Literal` types is crucial here. They serve to indicate the specific values these fields can take on, effectively constraining the model's output. Without these constraints, the model had only a 7.5% accuracy because it was free to generate arbitrary names for transaction types and categories.\n",
    "\n",
    "By providing this structure:\n",
    "1. We guide the model to choose from a predefined set of options.\n",
    "2. We make it easier for the parser to validate the model's output.\n",
    "3. We reduce the likelihood of the model generating creative but incorrect category names.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff381f8",
   "metadata": {},
   "source": [
    "This approach should significantly improve our accuracy, as the model now has a clear, limited set of options to choose from, rather than an open-ended text generation task. It aligns the model's output more closely with our expected categorization scheme, which should lead to better matching with our reference data.\n",
    "\n",
    "In the next steps, we'll update our prompt to incorporate this new structure and re-run our analysis to see the improvement in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25eebb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an output parser\n",
    "parser = PydanticOutputParser(pydantic_object=EnrichedTransactionInformation)\n",
    "\n",
    "# Define the prompt template\n",
    "template = \"\"\"\n",
    "You are an expert financial assistant. Categorize the following transaction description.\n",
    "\n",
    "Transaction: {transaction_description}\n",
    "\n",
    "Provide the transaction_type and transaction_category.\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "# Get format instructions\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "# Create the prompt\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"transaction_description\"],\n",
    "    template=template,\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1028329",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a55d7c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5b80815",
   "metadata": {},
   "source": [
    "### Creating a Structured Prompt with Output Parser\n",
    "\n",
    "Now that we have defined our `EnrichedTransactionInformation` model, we'll create a structured prompt using a `PydanticOutputParser`. This will help guide the language model to produce outputs that conform to our specified structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e725fc5",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "In this code block, we've set up a structured prompt that incorporates our `EnrichedTransactionInformation` model:\n",
    "\n",
    "1. We created a `PydanticOutputParser` using our `EnrichedTransactionInformation` model. This parser will help ensure that the model's output adheres to our defined structure.\n",
    "\n",
    "2. We defined a prompt template that includes placeholders for the transaction description and format instructions.\n",
    "\n",
    "3. We obtained the format instructions from the parser. These instructions will guide the model on how to structure its output.\n",
    "\n",
    "4. Finally, we created a `PromptTemplate` that combines our template with the format instructions.\n",
    "\n",
    "This structured approach offers several benefits:\n",
    "\n",
    "- It provides clear guidance to the language model on the expected output format.\n",
    "- It ensures that the model's responses will be consistent with our predefined transaction types and categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa029f69",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "- It simplifies the parsing of the model's output, as we can expect it to conform to our `EnrichedTransactionInformation` structure.\n",
    "\n",
    "By using this structured prompt, we're likely to see a significant improvement in the accuracy of our transaction categorization. The model now has a clear framework for its responses, which should align much more closely with our expected categories and types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d77c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the chain\n",
    "gpt4_chain = prompt | gpt4_model | parser\n",
    "\n",
    "# Apply the chain to each transaction\n",
    "transaction_types = []\n",
    "transaction_categories = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    input_variables = {\n",
    "        \"transaction_description\": row[\"Transaction Description\"]\n",
    "    }\n",
    "    result = gpt4_chain.invoke(input_variables)\n",
    "    transaction_types.append(result.transaction_type)\n",
    "    transaction_categories.append(result.transaction_category)\n",
    "\n",
    "# Add results to dataframe\n",
    "df[\"transaction_type\"] = transaction_types\n",
    "df[\"transaction_category\"] = transaction_categories\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24dc4dd",
   "metadata": {},
   "source": [
    "In the next steps, we'll use this prompt to generate categorizations for our transactions and evaluate the improvement in accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ca2c7f",
   "metadata": {},
   "source": [
    "## Applying GPT-4 Model with Structured Output\n",
    "\n",
    "Now that we have our structured prompt and parser, we'll use them with the GPT-4 model to categorize our transactions. This approach should yield more accurate and consistent results compared to our previous attempt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12ecb70",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "In this code block, we've applied our structured approach using the GPT-4 model:\n",
    "\n",
    "1. We created a chain that combines our prompt, the GPT-4 model, and our parser. This chain will process each transaction description and output structured results.\n",
    "\n",
    "2. We iterated through each row in our dataframe, using the transaction description as input for our chain.\n",
    "\n",
    "3. For each transaction, we invoked the chain, which prompted GPT-4 with our structured prompt and parsed the output according to our `EnrichedTransactionInformation` model.\n",
    "\n",
    "4. We collected the results (transaction types and categories) and added them as new columns to our dataframe.\n",
    "\n",
    "5. Finally, we displayed the first few rows of the updated dataframe to verify the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900e460e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "This approach offers several advantages:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5582468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GPT-3.5 Turbo Model for evaluation\n",
    "gpt35_model = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9080b19d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "- The use of GPT-4, combined with our structured prompt and parser, should provide more accurate and consistent categorizations.\n",
    "- The output is guaranteed to conform to our predefined transaction types and categories, eliminating the possibility of arbitrary or unexpected values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09c467a",
   "metadata": {},
   "source": [
    "- We can directly compare these results with our previous attempts or with any reference data we might have.\n",
    "\n",
    "In the next steps, we'll evaluate the accuracy of these new categorizations. We should expect to see a significant improvement compared to our earlier results, given the more structured approach and the use of the more capable GPT-4 model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d30ddb",
   "metadata": {},
   "source": [
    "## Part 3: Setup the OpenAI GPT-3.5 Turbo Model for Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7af02a",
   "metadata": {},
   "source": [
    "## Part 4: Evaluate Model with GPT-3.5 Turbo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef077308",
   "metadata": {},
   "source": [
    "### Applying GPT-3.5-Turbo Model to Transactions\n",
    "\n",
    "We will now use the GPT-3.5-Turbo model to categorize our transactions. This process involves creating a chain that combines our prompt, the GPT-3.5-Turbo model, and a parser. We'll then apply this chain to each transaction in our dataset and store the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0edc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the chain\n",
    "gpt35_chain = prompt | gpt35_model | parser\n",
    "\n",
    "# Apply the chain to each transaction\n",
    "transaction_types = []\n",
    "transaction_categories = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    input_variables = {\n",
    "               \"transaction_description\": row[\"Transaction Description\"]\n",
    "   }\n",
    "   result = gpt35_chain.invoke(input_variables)\n",
    "   transaction_types.append(result.transaction_type)\n",
    "   transaction_categories.append(result.transaction_category)\n",
    "\n",
    "#  Add results to dataframe\n",
    "df[\"gpt35_transaction_type\"] = transaction_types\n",
    "df[\"gpt35_transaction_category\"] = transaction_categories\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87719e59",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "In this code block, we created a chain combining our prompt, the GPT-3.5-Turbo model, and a parser. We then applied this chain to each transaction in our dataset. For each transaction, we used its description as input and obtained the predicted transaction type and category.\n",
    "\n",
    "The results were stored in two new columns in our dataframe: `gpt35_transaction_type` and `gpt35_transaction_category`. We displayed the first few rows of the updated dataframe to verify the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31255777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate answers using LangChain evaluators\n",
    "from langchain.evaluation import load_evaluator, EvaluatorType\n",
    "\n",
    "evaluator = load_evaluator(EvaluatorType.EXACT_MATCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffd047b",
   "metadata": {},
   "source": [
    "\n",
    "This process allows us to compare the categorization performance of GPT-3.5-Turbo with other models or reference data we might have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3577d7f1",
   "metadata": {},
   "source": [
    "### LangChain Evaluators\n",
    "\n",
    "LangChain provides a set of evaluation tools to assess the performance of language models and chains. The `load_evaluator` function and `EvaluatorType` enum are key components of this evaluation framework.\n",
    "\n",
    "#### load_evaluator Function\n",
    "\n",
    "The `load_evaluator` function is used to load a specific evaluator based on the provided type. Here's its basic structure:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8bd7a2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "\n",
    "This function takes the following parameters:\n",
    "\n",
    "- `evaluator`: The type of evaluator to load (an `EvaluatorType` enum value)\n",
    "- `llm`: (Optional) A language model to use for evaluation -- e.g. to compare predicted vs desired outputs for similarity\n",
    "- `**kwargs`: Additional keyword arguments specific to the evaluator\n",
    "\n",
    "The function returns an instance of the requested evaluator, which can be used to perform evaluations.\n",
    "\n",
    "#### EvaluatorType\n",
    "\n",
    "`EvaluatorType` is an enumeration that defines various types of evaluators available in LangChain. Some of the key evaluator types include:\n",
    "\n",
    "- `EXACT_MATCH`: Compares predictions to a reference answer using exact matching\n",
    "- `QA`: Question answering evaluator that grades answers using an LLM\n",
    "- `COT_QA`: Chain of thought question answering evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4757b1f6",
   "metadata": {
    "id": "aBkLo3_L1O5x"
   },
   "source": [
    "- `CONTEXT_QA`: Question answering evaluator that incorporates context in the response\n",
    "- `CRITERIA`: Evaluates a model based on custom criteria without reference labels\n",
    "- `LABELED_CRITERIA`: Evaluates a model based on custom criteria with a reference label\n",
    "- `STRING_DISTANCE`: Compares predictions to a reference using string edit distances\n",
    "- `EMBEDDING_DISTANCE`: Compares predictions using embedding distances\n",
    "- `JSON_VALIDITY`: Checks if a prediction is valid JSON\n",
    "- `REGEX_MATCH`: Compares predictions to a reference using regular expressions\n",
    "\n",
    "Each evaluator type is designed for specific evaluation tasks, allowing you to choose the most appropriate method for your use case.\n",
    "\n",
    "In the example provided, `EvaluatorType.EXACT_MATCH` is used, which will create an evaluator that checks for exact matches between predictions and reference answers. This is particularly useful for assessing the accuracy of categorical predictions, such as transaction types and categories.\n",
    "\n",
    "See  https://api.python.langchain.com/en/latest/langchain/evaluation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f6f752",
   "metadata": {},
   "source": [
    "We will now evaluate the accuracy of our model's predictions for transaction types and categories using the exact match evaluator.\n",
    "python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd3df5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the dataframe and evaluate the predictions\n",
    "transaction_types = []\n",
    "transaction_categories = []\n",
    "\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    transaction_type_score = evaluator.evaluate_strings(\n",
    "        prediction=row.gpt35_transaction_category,\n",
    "        reference=row.transaction_type,\n",
    "    )\n",
    "\n",
    "    transaction_category_score = evaluator.evaluate_strings(\n",
    "        prediction=row.gpt35_transaction_category,\n",
    "        reference=row.transaction_category,\n",
    "    )\n",
    "\n",
    "    transaction_types.append(transaction_type_score)\n",
    "    transaction_categories.append(transaction_category_score)\n",
    "\n",
    "df[\"transaction_type_score\"] = transaction_types\n",
    "df[\"transaction_category_score\"] = transaction_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54799258",
   "metadata": {},
   "source": [
    "In this cell, we iterated through each row of our dataframe, comparing the model's predictions (`mistral_transaction_type` and `mistral_transaction_category`) against the reference values (`transaction_type` and `transaction_category`) using the exact match evaluator. We then added two new columns to our dataframe, `transaction_type_score` and `transaction_category_score`, which contain the evaluation scores for each prediction. These scores indicate whether the model's predictions exactly match the reference values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a086ab",
   "metadata": {
    "id": "jxDQ3UZl5W16"
   },
   "source": [
    "## Calculating Overall Accuracy\n",
    "\n",
    "We will now calculate the overall accuracy of our model's predictions. This score will combine the accuracy of both transaction type and category predictions, giving us a single metric to evaluate our model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc08a6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score = 0\n",
    "\n",
    "for transaction_type_score, transaction_category_score in zip(\n",
    "    transaction_types, transaction_categories\n",
    "):\n",
    "    accuracy_score += transaction_type_score['score'] + transaction_category_score['score']\n",
    "\n",
    "accuracy_score = accuracy_score / (len(transaction_types) * 2)\n",
    "print(f\"Accuracy score: {accuracy_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09129ccc",
   "metadata": {
    "id": "g1Zar8ts34mr"
   },
   "source": [
    "The accuracy score we obtained is surprisingly low:\n",
    "\n",
    "Accuracy score: 0.075\n",
    "\n",
    "This score indicates that our model's predictions match the reference values only 7.5% of the time, which is far from ideal. Such a low accuracy suggests that there might be significant issues with our current approach. Here are some potential next steps to investigate and improve our model:\n",
    "\n",
    "1. **Error Analysis**: Examine a sample of misclassified transactions to understand where the model is failing. Look for patterns in the errors.\n",
    "\n",
    "2. **Data Quality Check**: Verify the quality and consistency of our reference data. Ensure that the 'transaction_type' and 'transaction_category' columns are correctly labeled."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
