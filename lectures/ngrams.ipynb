{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da39805b",
   "metadata": {},
   "source": [
    "# N-Gram Language Modeling Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ba0be8",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this lab, you will build an n-gram language model using a corpus from Project Gutenberg. You'll start by loading and tokenizing the corpus using a pre-existing tokenizer, constructing an n-gram frequency dictionary, and finally implementing beam search to generate text based on your model. Some parts of the code will be left for you to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9fc16b",
   "metadata": {},
   "source": [
    "## 1. Load the Corpus\n",
    "\n",
    "First, we'll load a text corpus from Project Gutenberg. For this lab, we'll use the text of \"Alice's Adventures in Wonderland\" by Lewis Carroll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc8a95f5",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "\n",
    "def download_corpus(url, filename):\n",
    "    if not os.path.exists(filename):\n",
    "        urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa829a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_url = \"https://gutenberg.org/ebooks/11.txt.utf-8\"\n",
    "corpus_filename = \"alice_in_wonderland.txt\"\n",
    "\n",
    "download_corpus(corpus_url, corpus_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07d096e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open(corpus_filename, 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ff5884",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe11314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip out unicode (for this demo)\n",
    "corpus = corpus.encode('ascii', 'ignore').decode('utf-8')\n",
    "corpus[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445b511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import fill\n",
    "print(fill(corpus[:100], drop_whitespace=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67309f80",
   "metadata": {},
   "source": [
    "## 2. Tokenize the Corpus\n",
    "\n",
    "In this section, you'll build a simple Byte Pair Encoding (BPE) tokenizer to split the text into tokens. BPE is a subword segmentation algorithm that iteratively merges the most frequent pair of bytes or characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587a2608",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = list(corpus)\n",
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887dbe43",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_stats(tokens):\n",
    "    \"\"\"Compute frequencies of adjacent pairs.\"\"\"\n",
    "    pairs = defaultdict(int)\n",
    "    for i in range(len(tokens)-1):\n",
    "        pairs[(tokens[i], tokens[i+1])] +=1\n",
    "    return pairs\n",
    "\n",
    "stats = get_stats(tokens)\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f71afca",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "top_n_tokens = sorted(stats.items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "top_n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d529ca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_token = max(stats, key=stats.get)\n",
    "most_frequent_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49226e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vocab(pair, tokens):\n",
    "    merged = []\n",
    "    i = 0\n",
    "    while i < len(tokens)-1:\n",
    "        if (tokens[i], tokens[i+1]) == pair:\n",
    "            merged.append(pair)\n",
    "            i += 2\n",
    "        else:\n",
    "            merged.append(tokens[i])\n",
    "            i += 1\n",
    "    return merged\n",
    "\n",
    "tokens = merge_vocab(most_frequent_token, tokens)\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954306bf",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def flatten_token(token):\n",
    "    if isinstance(token, tuple):\n",
    "        return ''.join(flatten_token(t) for t in token)\n",
    "    else:\n",
    "        return token\n",
    "\n",
    "flatten_token((((' ', 't'), 'h'), 'e'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862afa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "import tqdm.notebook as tq\n",
    "\n",
    "def build_bpe_vocab(corpus, num_merges, verbose=False):\n",
    "    \"\"\"Build BPE vocabulary.\"\"\"\n",
    "    # Initialize tokens as characters with end-of-word token\n",
    "    tokens = list(corpus)\n",
    "    vocab_size = len(set(tokens))\n",
    "    \n",
    "    for i in tq.trange(num_merges, leave=False, desc='Building BPE vocab'):\n",
    "        pairs = get_stats(tokens)\n",
    "        if not pairs:\n",
    "            break\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        tokens = merge_vocab(best, tokens)\n",
    "        if verbose:\n",
    "            tq.tqdm.write(f'Merge {i+1:<5}: {repr(flatten_token(best)):<10}')\n",
    "    \n",
    "    vocab = sorted([flatten_token(t) for t  in set(tokens)], key=len, reverse=True)\n",
    "    return vocab\n",
    "\n",
    "vocab = build_bpe_vocab(corpus, num_merges=10, verbose=True)\n",
    "print(\"\\nBPE Vocabulary (first 10 tokens):\", vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec169abc",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "#vocab = build_bpe_vocab(corpus, num_merges=len(corpus) - 1000)\n",
    "vocab = build_bpe_vocab(corpus, num_merges=1000) # (I am not patient, this may take a while)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58488dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a50c3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lengh of final vocab:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ff60d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Consider using a Trie data structure for efficient tokenization (e.g. marisa_trie)\n",
    "def tokenize(input_str, vocab):\n",
    "    tokens = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(input_str):\n",
    "        # Try to match the longest token from the current position\n",
    "        for token in vocab:\n",
    "            if input_str[i:].startswith(token):\n",
    "                tokens.append(token)\n",
    "                i += len(token)\n",
    "                break\n",
    "        else:\n",
    "            tokens.append(token)\n",
    "            i += 1\n",
    "\n",
    "    return tokens\n",
    "\n",
    "bpe_tokens = tokenize(corpus, vocab) # Takes about 2m 30 seconds\n",
    "print(\"First 10 tokens:\", bpe_tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0933304",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b01f0aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install marisa-trie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7750b2e5",
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "import marisa_trie\n",
    "\n",
    "def tokenize(input_str, vocab):\n",
    "    # Build the Trie from the vocabulary\n",
    "    trie = marisa_trie.Trie(vocab)\n",
    "    \n",
    "    tokens = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(input_str):\n",
    "        # Find all prefixes that match the current position\n",
    "        prefixes = trie.prefixes(input_str[i:])\n",
    "        \n",
    "        if prefixes:\n",
    "            # Choose the longest matching prefix\n",
    "            longest_prefix = max(prefixes, key=len)\n",
    "            tokens.append(longest_prefix)\n",
    "            i += len(longest_prefix)  # Move forward by the length of the matched token\n",
    "        else:\n",
    "            tokens.append(input_str[i])\n",
    "            i += 1  # No match, move to the next character\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "bpe_tokens = tokenize(corpus, vocab) # Takes about 2.5 seconds\n",
    "print(\"First 10 tokens:\", bpe_tokens[:10])  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f314745",
   "metadata": {},
   "source": [
    "## 3. Create N-Gram Frequency Dictionary\n",
    "\n",
    "We'll create a hash table where each prefix (n-1 grams) maps to a dictionary of possible next tokens and their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9948e98a",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "def build_ngram_freq(tokens, n=2):\n",
    "    ngram_freq = defaultdict(Counter)\n",
    "    for k in range(1, n+1):\n",
    "        for i in range(len(tokens) - k):\n",
    "            ngram = tokens[i:][:k]\n",
    "            prefix = ngram[:-1]  \n",
    "            next_token = ngram[-1]\n",
    "            ngram_freq[tuple(prefix)][next_token] += 1\n",
    "    return ngram_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e138b8c9",
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "ngram_freq = build_ngram_freq(bpe_tokens, n=2)\n",
    "print(f\"Number of unique prefixes: {len(ngram_freq)}\")\n",
    "print(\"Sample prefix and possible next tokens with frequencies:\")\n",
    "sample_prefix = list(ngram_freq.keys())[0]\n",
    "print(f\"Prefix: {sample_prefix}\")\n",
    "print(ngram_freq[sample_prefix].most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4953ac11",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "\n",
    "```txt\n",
    "    Number of unique prefixes: 1044\n",
    "    Sample prefix and possible next tokens with frequencies:\n",
    "    Prefix: ('The ',)\n",
    "    [('Queen', 4), ('Mouse ', 4), ('Queen ', 4), ('P', 3), ('Rabbit ', 3)]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc85789d",
   "metadata": {},
   "source": [
    "## 4. Beam Search Sampling\n",
    "\n",
    "We'll implement beam search to generate text based on the n-gram model. Beam search keeps track of the top `k` sequences at each step to explore multiple hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1b46593e",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "def beam_search_sampling(ngram_freq, n=2, beam_width=3, max_length=20, seed_prefix=None):\n",
    "    \"\"\"\n",
    "    Generate text using beam search.\n",
    "    \n",
    "    Args:\n",
    "        ngram_freq (dict): The n-gram frequency dictionary.\n",
    "        beam_width (int): Number of sequences to keep at each step.\n",
    "        max_length (int): Maximum number of tokens to generate.\n",
    "        seed_prefix (list): Initial list of tokens to start the generation.\n",
    "        \n",
    "    Returns:\n",
    "        list: The generated sequence of tokens.\n",
    "    \"\"\"\n",
    "    if seed_prefix is None:\n",
    "        # Start with a random prefix\n",
    "        seed_prefix = []\n",
    "\n",
    "    if not isinstance(seed_prefix, list):\n",
    "        seed_prefix = [seed_prefix]\n",
    "\n",
    "    sequences = [(seed_prefix, 0.0)]  # (sequence, score)\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        all_candidates = []\n",
    "        for seq, score in sequences:\n",
    "            prefix = seq[-n+1:]  # For an n-gram model, the prefix is the last n-1 tokens\n",
    "            next_tokens = ngram_freq.get(tuple(prefix), {})\n",
    "            if not next_tokens:\n",
    "                continue\n",
    "            for token, freq in next_tokens.items():\n",
    "                prob = freq / sum(next_tokens.values())\n",
    "                candidate_seq = seq + [token]  \n",
    "                candidate_score = score - math.log(prob)\n",
    "                all_candidates.append((candidate_seq, candidate_score))\n",
    "        if len(all_candidates) == 0:\n",
    "            break  # No more extensions, stop generating\n",
    "\n",
    "        # Sort all candidates by score and select top beam_width\n",
    "        ordered = sorted(all_candidates, key=lambda tup: tup[1])\n",
    "        sequences = ordered[:beam_width]\n",
    "        \n",
    "        # Stop if all sequences have no extensions\n",
    "        if not sequences:\n",
    "            break\n",
    "    \n",
    "    # Select the sequence with the lowest score\n",
    "    if len(sequences) == 0:\n",
    "        return []\n",
    "    else:\n",
    "        best_sequence,_ = min(sequences, key=lambda tup: tup[1])\n",
    "        return best_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268fc632",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_tokens = beam_search_sampling(ngram_freq, n=2, beam_width=5, max_length=20)\n",
    "\n",
    "print(\"\\nGenerated Text:\")\n",
    "print(''.join(generated_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85abaf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "ngram4_freq = build_ngram_freq(bpe_tokens, n=n)\n",
    "generated_tokens = beam_search_sampling(ngram4_freq, n=n, beam_width=5, max_length=20)\n",
    "\n",
    "print(\"\\nGenerated Text:\")\n",
    "print(''.join(generated_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cdb223",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_tokens = beam_search_sampling(ngram4_freq, n=n, beam_width=5, max_length=100)\n",
    "print(\"\\nGenerated Text:\")\n",
    "print(''.join(generated_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e744331e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "custom_cell_magics": "kql",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "cse434",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
