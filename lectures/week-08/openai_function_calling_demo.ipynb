{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e6cbef8",
   "metadata": {},
   "source": [
    "# OpenAI Function Calling with LangChain\n",
    "\n",
    "**Learning Objectives**:\n",
    "- Understand the concept of OpenAI Function Calling.\n",
    "- Learn how to define functions and their JSON schemas.\n",
    "- Explore how to integrate function calling with LangChain using structured output parsers.\n",
    "- Demonstrate handling function calls and processing responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015c53a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai langchain langchain-openai python-dotenv --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6995efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "from os import getenv\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from pprint import pprint\n",
    "import ast\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc60c729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117bb0ef",
   "metadata": {},
   "source": [
    "## Function Callse _Without_ LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2129ea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the schedule_meeting function\n",
    "def schedule_meeting(date: str, time: str, attendees: List[str]):\n",
    "    # Connect to calendar service (mock implementation)\n",
    "    return {\n",
    "        \"event_id\": \"1234\",\n",
    "        \"status\": \"Meeting scheduled successfully!\",\n",
    "        \"date\": date,\n",
    "        \"time\": time,\n",
    "        \"attendees\": attendees\n",
    "    }\n",
    "\n",
    "# Map function names to functions\n",
    "OPENAI_FUNCTIONS = {\n",
    "    \"schedule_meeting\": schedule_meeting\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1cf244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the OpenAI (not LangChain) way to define tools / functions\n",
    "# Define the JSON schema for schedule_meeting\n",
    "# see https://platform.openai.com/docs/guides/function-calling\n",
    "functions = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"type\": \"object\",\n",
    "            \"name\": \"schedule_meeting\",\n",
    "            \"description\": \"Set a meeting at a specified date and time for designated attendees\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"date\": {\"type\": \"string\", \"format\": \"date\"},\n",
    "                    \"time\": {\"type\": \"string\", \"format\": \"time\"},\n",
    "                    \"attendees\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                },\n",
    "                \"required\": [\"date\", \"time\", \"attendees\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0c3d55",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- **JSON Schema**: Defines the structure of the function call, specifying the required parameters and their types. This schema helps the model understand how to format the function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a33556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are still using the raw OpenAI API (not LangChain)\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI(api_key=getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Start the conversation with a user request\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": '''Schedule a meeting on 2023-11-01 at 14:00 with Alice and Bob.''',\n",
    "    }\n",
    "]\n",
    "\n",
    "# Send the conversation and function schema to the model\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    messages=messages,\n",
    "    tools=functions,\n",
    ")\n",
    "\n",
    "response = response.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c369fdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model should return the specs for a tool (function)\n",
    "assert response.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330c0a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2d5a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the first function call\n",
    "first_tool_call = response.tool_calls[0]\n",
    "\n",
    "# Extract function name and arguments\n",
    "function_name = first_tool_call.function.name\n",
    "function_args = json.loads(first_tool_call.function.arguments)\n",
    "\n",
    "print(\"This is the function name: \", function_name)\n",
    "print(\"These are the function arguments: \", function_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5625848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fdf6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the function from the mapping\n",
    "function = OPENAI_FUNCTIONS.get(function_name)\n",
    "\n",
    "if not function:\n",
    "    raise Exception(f\"Function {function_name} not found.\")\n",
    "\n",
    "function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5806f39",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Call the function with the extracted arguments\n",
    "function_response = function(**function_args)\n",
    "function_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf8b6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the function's response to the messages\n",
    "messages.append(\n",
    "    {\n",
    "        \"role\": \"function\", # <-- The other roles have been \"Assistant\" and \"User\"\n",
    "        \"name\": function_name,\n",
    "        \"content\": json.dumps(function_response),\n",
    "    }\n",
    ")\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a210400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let the model generate a user-friendly response\n",
    "second_response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(second_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2827e5",
   "metadata": {},
   "source": [
    "### Function Calling with Multiple Function Calls\n",
    "\n",
    "Function calling can handle multiple function invocations within a single user request. Let's demonstrate this by scheduling two meetings in one conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a227604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the conversation with a user request for two meetings\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            '''Schedule a meeting on 2023-11-01 at 14:00 with Alice and Bob. '''\n",
    "            '''Then I want to schedule another meeting on 2023-11-02 at 15:00 with Charlie and Dave.'''),\n",
    "    }\n",
    "]\n",
    "\n",
    "# Send the conversation and function schema to the model\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=messages,\n",
    "    tools=functions,\n",
    ")\n",
    "\n",
    "response = response.choices[0].message\n",
    "\n",
    "for i, tool in enumerate(response.tool_calls, start=1):\n",
    "    print(f\"{i} : {tool.function}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcbfd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model wants to call functions\n",
    "if response.tool_calls:\n",
    "    for tool_call in response.tool_calls:\n",
    "        # Extract function name and arguments\n",
    "        function_name = tool_call.function.name\n",
    "        function_args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "        # Retrieve the function from the mapping\n",
    "        function = OPENAI_FUNCTIONS.get(function_name)\n",
    "\n",
    "        if not function:\n",
    "            raise Exception(f\"Function {function_name} not found.\")\n",
    "\n",
    "        # Call the function with the extracted arguments\n",
    "        # - Technically not in parallel but _could_ be\n",
    "        function_response = function(**function_args)\n",
    "\n",
    "        # Append the function's response to the messages\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"function\",\n",
    "                \"name\": function_name,\n",
    "                \"content\": json.dumps(function_response),\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "    # Let the model generate a user-friendly response\n",
    "    second_response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    print(second_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0a75e2",
   "metadata": {},
   "source": [
    "# Function Calling in LangChain\n",
    "\n",
    "If youâ€™d prefer to avoid writing JSON schemas and simply want to extract structured data from an LLM response, LangChain allows you to use function calling with Pydantic. This approach leverages `PydanticToolsParser` to parse the model's responses into defined Pydantic models, ensuring structured and reliable data extraction.\n",
    "on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a250c4",
   "metadata": {},
   "source": [
    "The textbook is stale -- this has the right instructions for langchain  \n",
    "\n",
    "https://python.langchain.com/v0.1/docs/modules/model_io/chat/function_calling/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0081401",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.openai_tools import PydanticToolsParser\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230f0346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "\n",
    "tools = [add, multiply]\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7a7d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c18290",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12eb03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is 3 * 12? Also, what is 11 + 49?\"\n",
    "\n",
    "llm_with_tools.invoke(query).tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aff457",
   "metadata": {},
   "source": [
    "There is little difference between a tool 'call' and esxtracting data.\n",
    "\n",
    "We can use pydantic to create classes represnting the tools (functions to call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd5b0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Note that the docstrings here are crucial, as they will be passed along\n",
    "# to the model along with the class name.\n",
    "class add(BaseModel):\n",
    "    \"\"\"Add two integers together.\"\"\"\n",
    "\n",
    "    a: int = Field(..., description=\"First integer\")\n",
    "    b: int = Field(..., description=\"Second integer\")\n",
    "\n",
    "\n",
    "class multiply(BaseModel):\n",
    "    \"\"\"Multiply two integers together.\"\"\"\n",
    "\n",
    "    a: int = Field(..., description=\"First integer\")\n",
    "    b: int = Field(..., description=\"Second integer\")\n",
    "\n",
    "\n",
    "tools = [add, multiply]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c374ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "chain = llm_with_tools | PydanticToolsParser(tools=tools)\n",
    "chain.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03963156",
   "metadata": {},
   "source": [
    "Passing results back to the model (ToolMessage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d27185e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "tools = [add, multiply]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "messages = [HumanMessage(query)]\n",
    "ai_msg = llm_with_tools.invoke(messages)\n",
    "messages.append(ai_msg)\n",
    "\n",
    "TOOL_FUNCS = {\n",
    "    'add': add,\n",
    "    'multiply': multiply\n",
    "}\n",
    "\n",
    "for tool_call in ai_msg.tool_calls:\n",
    "    selected_tool = tool_call[\"name\"].lower()\n",
    "\n",
    "    # Use 'invoke' because they are now tools, not functions.\n",
    "    tool_output = TOOL_FUNCS[selected_tool].invoke(tool_call[\"args\"])\n",
    "\n",
    "    messages.append(ToolMessage(tool_output, tool_call_id=tool_call[\"id\"]))\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cfe095",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2991435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Pydantic model for structured data extraction\n",
    "class Article(BaseModel):\n",
    "    \"\"\"Identifying key points and contrarian views in an article.\"\"\"\n",
    "\n",
    "    points: str = Field(..., description=\"Key points from the article\")\n",
    "    contrarian_points: Optional[str] = Field(\n",
    "        None, description=\"Any contrarian points acknowledged in the article\"\n",
    "    )\n",
    "    author: Optional[str] = Field(None, description=\"Author of the article\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad93665a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "_EXTRACTION_TEMPLATE = \"\"\"Extract and save the relevant entities mentioned\n",
    "in the following passage together with their properties.\n",
    "\n",
    "If a property is not present and is not required in the function parameters,\n",
    "do not include it in the output.\"\"\"\n",
    "\n",
    "# Create a prompt telling the LLM to extract information\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage( _EXTRACTION_TEMPLATE),\n",
    "        HumanMessage(\"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize the ChatOpenAI model\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# Define Pydantic schemas\n",
    "pydantic_schemas = [Article]\n",
    "\n",
    "tools = [convert_to_openai_tool(schema) for schema in pydantic_schemas]\n",
    "\n",
    "# Bind the tools directly to the LLM\n",
    "model = model.bind_tools(tools=pydantic_schemas)\n",
    "\n",
    "# Create an end-to-end chain with the parser\n",
    "chain = prompt | model | PydanticToolsParser(tools=pydantic_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71aaa5e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Example input text\n",
    "input_text = \"\"\"In the recent article titled 'AI adoption in industry,'\n",
    "key points addressed include the growing interest in AI technologies across various sectors. However, the\n",
    "author, Dr. Jane Smith, emphasizes the need for stringent regulations to prevent misuse.\"\"\"\n",
    "\n",
    "# Invoke the chain with the input text\n",
    "result = chain.invoke(\n",
    "    {\n",
    "        \"input\": input_text\n",
    "    }\n",
    ")\n",
    "\n",
    "# Print the structured extraction result\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "custom_cell_magics": "kql",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
